{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSfO3DFqz6hw/I1vDpYxYi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Narrow Transformations"],"metadata":{"id":"9DI8gVzbayIw"}},{"cell_type":"markdown","source":["1. Map: Applying a function to each element in the data set."],"metadata":{"id":"n9D6JfPFcDzj"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O0y1y8hails","executionInfo":{"status":"ok","timestamp":1745995387518,"user_tz":420,"elapsed":10664,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"ab726753-1b11-4ae8-82b4-08ecd4fae45c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4, 6, 8, 10]"]},"metadata":{},"execution_count":3}],"source":["from pyspark import SparkContext\n","sc = SparkContext(\"local\", \"MapExample\")\n","data = [1, 2, 3, 4, 5]\n","rdd = sc.parallelize(data)\n","mapped_rdd = rdd.map(lambda x: x* 2)\n","mapped_rdd.collect() # Output: [2, 4, 6, 8, 10]"]},{"cell_type":"markdown","source":["2. Filter: Selecting elements based on a specified condition"],"metadata":{"id":"WVg_SuSbb4W4"}},{"cell_type":"code","source":["# from pyspark import SparkContext\n","# sc = SparkContext(\"local\", \"FilterExample\")\n","data = [1, 2, 3, 4, 5]\n","rdd = sc.parallelize(data)\n","filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n","filtered_rdd.collect() # Output: [2, 4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUMVoYEUcPY1","executionInfo":{"status":"ok","timestamp":1745995695267,"user_tz":420,"elapsed":270,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"26f069fc-4aa7-434f-abb1-beb9612e20fa"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["3. Union: Combining two data sets with the same schema"],"metadata":{"id":"tpW_3TXqdCJL"}},{"cell_type":"code","source":["# from pyspark import SparkContext\n","# sc = SparkContext(\"local\", \"FilterExample\")\n","rdd1 = sc.parallelize([1, 2, 3])\n","rdd2 = sc.parallelize([4, 5, 6])\n","union_rdd = rdd1.union(rdd2)\n","union_rdd.collect() # Output: [1, 2, 3, 4, 5, 6]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjNjCb0pb3hd","executionInfo":{"status":"ok","timestamp":1745995865448,"user_tz":420,"elapsed":144,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"33436f29-2026-483d-a671-2b0c9c61e420"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5, 6]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# Wide Transformations"],"metadata":{"id":"dICyw0MFdsyt"}},{"cell_type":"markdown","source":["Wide transformations can be compared to tasks accomplished with teamwork and where information is needed from different groups to conclude. Imagine you have a group of friends, each with a puzzle piece. In order to put the puzzle together, you might need to trade pieces between your friends to make everything fit. These kinds of tasks are a good example of wide transformation. Such tasks can be a little more complicated because everyone needs to collaborate and move pieces around."],"metadata":{"id":"gQLG3Pfxd2rA"}},{"cell_type":"markdown","source":["1. GroupBy: Aggregating data based on a specific key."],"metadata":{"id":"8HGlSiRoeK4v"}},{"cell_type":"code","source":["# from pyspark import SparkContext\n","# sc = SparkContext(\"local\", \"GroupByExample\")\n","data = [(\"apple\", 2), (\"banana\", 3), (\"apple\", 5), (\"banana\", 1)]\n","rdd = sc.parallelize(data)\n","grouped_rdd = rdd.groupBy(lambda x: x[0])\n","sum_rdd = grouped_rdd.mapValues(lambda values: sum([v[1] for v in values]))\n","sum_rdd.collect() # Output: [('apple', 7), ('banana',4)]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUZizOhkdv0A","executionInfo":{"status":"ok","timestamp":1745996500960,"user_tz":420,"elapsed":1606,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"920aede9-6a76-4cbc-cb66-0e39b1843175"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('apple', 7), ('banana', 4)]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["2. Join: Combining two data sets based on a common key"],"metadata":{"id":"5DGtkjvUgGUS"}},{"cell_type":"code","source":["# from pyspark import SparkContext\n","# sc = SparkContext(\"local\", \"JoinExample\")\n","rdd1= sc.parallelize([(\"apple\", 2), (\"banana\", 3)])\n","rdd2 = sc.parallelize([(\"apple\", 5), (\"banana\", 1)])\n","joined_rdd = rdd1.join(rdd2)\n","joined_rdd.collect() # Output: [('apple', (2, 5)), ('banana', (3, 1))]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRNci4VkgNFb","executionInfo":{"status":"ok","timestamp":1745997124913,"user_tz":420,"elapsed":1907,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"a4c490a9-8db8-4cf1-9982-e312b17698a3"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('apple', (2, 5)), ('banana', (3, 1))]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["3. Sort: Rearranging data based on a specific criterion"],"metadata":{"id":"hw_QaBpiif2P"}},{"cell_type":"code","source":["# from pyspark import SparkContext\n","# sc = SparkContext(\"local\", \"SortExample\")\n","data = [4, 2, 1, 3, 5]\n","rdd = sc.parallelize(data)\n","sorted_rdd = rdd.sortBy(lambda x: x, ascending=True)\n","sorted_rdd.collect() # Output: [1, 2, 3, 4, 5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsmcUbgeiniC","executionInfo":{"status":"ok","timestamp":1745997351945,"user_tz":420,"elapsed":365,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"96386d9b-56b3-4f06-e099-bccb79eb2187"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# PySpark DataFrame: Rule-based common transformation"],"metadata":{"id":"wPfAgwLujXaz"}},{"cell_type":"markdown","source":["1. Predicate pushdown: Pushing filtering conditions closer to the data source before processing to minimize data movement.\n","2. Constant folding:  Evaluating constant expressions during query compilation to reduce computation during runtime.\n","3. Column pruning: Eliminating unnecessary columns from the query plan to enhance processing efficientcy.\n","4. Join reordering: Rearranging join operations to minimize the intermediate data size and enhance the join performance."],"metadata":{"id":"WmXxv_Hgq3_H"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"ruleBasedTransformations\").getOrCreate()\n","\n","# Sample input data for DataFrame 1\n","data1 = [\n","    (\"Alice\", 25, \"F\"),\n","    (\"Bob\", 30, \"M\"),\n","    (\"Charlie\", 22, \"M\"),\n","    (\"Diana\", 28, \"F\")\n","]\n","\n","# Sample input data for DataFrames 2\n","data2 = [\n","    (\"Alice\", \"New York\"),\n","    (\"Bob\", \"San Francisco\"),\n","    (\"Charlie\", \"Los Angeles\"),\n","    (\"Eve\", \"Chicago\")\n","]\n","\n","# Create DataFrames\n","columns1 = [\"name\", \"age\", \"gender\"]\n","df1 = spark.createDataFrame(data1, columns1)\n","\n","columns2 = [\"name\", \"city\"]\n","df2 = spark.createDataFrame(data2, columns2)\n","\n","# Applying Predicate Pyshdown (Filtering)\n","filtered_df = df1.filter(col(\"age\") > 25)\n","\n","# Applying Constant Folding\n","folded_df = filtered_df.select(col(\"name\"), col(\"age\") + 2)\n","\n","# Applying Column Pruning\n","pruned_df = folded_df.select(col(\"name\"))\n","\n","# Join Reordering\n","reordered_join = df1.join(df2, on=\"name\")\n","\n","# Show the final results\n","print(\"Filtered DataFrame:\")\n","filtered_df.show()\n","\n","print(\"Folded DataFrame:\")\n","folded_df.show()\n","\n","print(\"Pruned DataFrame:\")\n","pruned_df.show()\n","\n","print(\"reordered DataFrame:\")\n","reordered_join.show()\n","\n","# Stop the Spark session\n","spark.stop()\n","spark = SparkSession.builder.appName(\"ruleBasedTransformations\").getOrCreate()\n","\n","# Sample input data for DataFrame 1\n","data1 = [\n","    (\"Alice\", 25, \"F\"),\n","    (\"Bob\", 30, \"M\"),\n","    (\"Charlie\", 22, \"M\"),\n","    (\"Diana\", 28, \"F\")\n","]\n","\n","# Sample input data for DataFrames 2\n","data2 = [\n","    (\"Alice\", \"New York\"),\n","    (\"Bob\", \"San Francisco\"),\n","    (\"Charlie\", \"Los Angeles\"),\n","    (\"Eve\", \"Chicago\")\n","]\n","\n","# Create DataFrames\n","columns1 = [\"name\", \"age\", \"gender\"]\n","df1 = spark.createDataFrame(data1, columns1)\n","\n","columns2 = [\"name\", \"city\"]\n","df2 = spark.createDataFrame(data2, columns2)\n","\n","# Applying Predicate Pyshdown (Filtering)\n","filtered_df = df1.filter(col(\"age\") > 25)\n","\n","# Applying Constant Folding\n","folded_df = filtered_df.select(col(\"name\"), col(\"age\") + 2)\n","\n","# Applying Column Pruning\n","pruned_df = folded_df.select(col(\"name\"))\n","\n","# Join Reordering\n","reordered_join = df1.join(df2, on=\"name\")\n","\n","# Show the final results\n","print(\"Filtered DataFrame:\")\n","filtered_df.show()\n","\n","print(\"Folded DataFrame:\")\n","folded_df.show()\n","\n","print(\"Pruned DataFrame:\")\n","pruned_df.show()\n","\n","print(\"reordered DataFrame:\")\n","reordered_join.show()\n","\n","# Stop the Spark session\n","spark.stop()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-pSTKwPjgqk","executionInfo":{"status":"ok","timestamp":1746051134714,"user_tz":420,"elapsed":20866,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"43b9e286-b51b-4a72-a3cc-35c0cdb2d6d9"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered DataFrame:\n","+-----+---+------+\n","| name|age|gender|\n","+-----+---+------+\n","|  Bob| 30|     M|\n","|Diana| 28|     F|\n","+-----+---+------+\n","\n","Folded DataFrame:\n","+-----+---------+\n","| name|(age + 2)|\n","+-----+---------+\n","|  Bob|       32|\n","|Diana|       30|\n","+-----+---------+\n","\n","Pruned DataFrame:\n","+-----+\n","| name|\n","+-----+\n","|  Bob|\n","|Diana|\n","+-----+\n","\n","reordered DataFrame:\n","+-------+---+------+-------------+\n","|   name|age|gender|         city|\n","+-------+---+------+-------------+\n","|  Alice| 25|     F|     New York|\n","|    Bob| 30|     M|San Francisco|\n","|Charlie| 22|     M|  Los Angeles|\n","+-------+---+------+-------------+\n","\n","Filtered DataFrame:\n","+-----+---+------+\n","| name|age|gender|\n","+-----+---+------+\n","|  Bob| 30|     M|\n","|Diana| 28|     F|\n","+-----+---+------+\n","\n","Folded DataFrame:\n","+-----+---------+\n","| name|(age + 2)|\n","+-----+---------+\n","|  Bob|       32|\n","|Diana|       30|\n","+-----+---------+\n","\n","Pruned DataFrame:\n","+-----+\n","| name|\n","+-----+\n","|  Bob|\n","|Diana|\n","+-----+\n","\n","reordered DataFrame:\n","+-------+---+------+-------------+\n","|   name|age|gender|         city|\n","+-------+---+------+-------------+\n","|  Alice| 25|     F|     New York|\n","|    Bob| 30|     M|San Francisco|\n","|Charlie| 22|     M|  Los Angeles|\n","+-------+---+------+-------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Cost-Based optimization techniques in Spark"],"metadata":{"id":"6pAxa3LRyAT8"}},{"cell_type":"markdown","source":["Spark employs cost-based optimization techniques to enhance the efficiency of query execution.  These methods involve estimating and analyzing the costs associated with queries, leading to more informed decisions that result in imporved performance.\n","\n","1. Adative query execution: Dynamically adjusts the query plan during execution based on runtime statistics to optimize performance.\n","2. Cost-based join reordering: Optimizes join order based on estiated costs of different join paths.\n","3. Boradcast hash join: Optimizes small-table joins by broadcasting one table to all nodes, reducing data shuffling.\n","4. Shuffle partitioning and memory management: Efficiently manages data shuffling during operations like groupBy and aggregation and optimizes memory usage.\n"],"metadata":{"id":"6WqQ6fkayL54"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"CostBaseOptimization\").getOrCreate()\n","\n","# Sample input data for DataFrame 1\n","data1 = [\n","    (\"Alice\", 25),\n","    (\"Bob\", 30),\n","    (\"Charlie\", 22),\n","    (\"Diana\", 28)\n","]\n","\n","# Sample input data for DataFrame 2\n","data2 = [\n","    (\"Alice\", \"New York\"),\n","    (\"Bob\", \"San Francisco\"),\n","    (\"Charlie\", \"Los Angeles\"),\n","    (\"Eve\", \"Chicago\")\n","]\n","\n","# Create DataFrames\n","columns1 = [\"name\", \"age\"]\n","df1 = spark.createDataFrame(data1, columns1)\n","\n","columns2 = [\"name\", \"city\"]\n","df2 = spark.createDataFrame(data2, columns2)\n","\n","# Enable adaptive query execution\n","spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n","\n","# Applying Adaptive Query Execution (Runtime adaptive optimization)\n","optimized_join = df1.join(df2, on=\"name\")\n","\n","# Show the optimized join result\n","print(\"optimized Join DataFrame:\")\n","optimized_join.show()\n","\n","# Stop the Spark session\n","spark.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWtPYgcCz2mU","executionInfo":{"status":"ok","timestamp":1746052651325,"user_tz":420,"elapsed":3985,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"4d633834-ac3d-4a7a-e72e-5815cffa6b1d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["optimized Join DataFrame:\n","+-------+---+-------------+\n","|   name|age|         city|\n","+-------+---+-------------+\n","|  Alice| 25|     New York|\n","|    Bob| 30|San Francisco|\n","|Charlie| 22|  Los Angeles|\n","+-------+---+-------------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"c63IcuQTx-SN"},"execution_count":null,"outputs":[]}]}