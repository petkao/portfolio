{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1STeIN_fRvlfljeMvd8KpsZGMtTrRuh85","authorship_tag":"ABX9TyOnyx+XL7SFXufUzyyA+nir"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["CNN\n","\n","This section covers:\n","\n","Convolutional Layers\n","MaxPooling\n","Save/Load model\n"],"metadata":{"id":"fEBwCK6KFGzb"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","num_epochs = 10\n","batch_size = 32\n","learning_rate = 0.001\n","\n","# dataset has PILImage images of range [0, 1]\n","# We transform them to Tensor of normalized range [-1, 1]\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                              download=True, transform=transform)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                            download=True, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n","                                          shuffle=False)\n","\n","\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","def imshow(imgs):\n","    imgs = imgs / 2 + 0.5\n","    npimgs = imgs.numpy()\n","    plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n","    plt.show()\n","\n","# one batch of random training imagges\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","img_grid = torchvision.utils.make_grid(images[0:25], nrow=5)\n","imshow(img_grid)\n","\n"],"metadata":{"id":"a9PDn_TJGXcN","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1708327870321,"user_tz":480,"elapsed":1991,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"7987342a-eb86-4742-da6f-5c21609d3d11"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'DataLoader' object has no attribute 'size'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-f655e707a473>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                           shuffle=False)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m classes = ('plane', 'car', 'bird', 'cat',\n","\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'size'"]}]},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(32, 64, 3)\n","        self.conv3 = nn.Conv2d(64, 64, 3)\n","        self.fc1 = nn.Linear(64*4*4, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","      # N, 3, 32, 32\n","      # print(x.shape)\n","      x = self.relu(self.conv1(x))  #  -> N, 32, 30, 30\n","      #print(x.shape)\n","      x = self.pool(x)           #  -> N, 32, 15, 15\n","      x = self.relu(self.conv2(x))  #  -> N, 64, 13, 13\n","      x = self.pool(x)           #  -> N, 64, 6, 6\n","      x = self.relu(self.conv3(x))  #  -> N, 64, 4, 4\n","      x = torch.flatten(x, 1)    #  -> N, 1024\n","      x = self.relu(self.fc1(x))    #  -> N, 64\n","      x = self.fc2(x)            #  -> N, 10\n","      return x\n","\n","\n","model = ConvNet().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","\n","    running_loss = 0.0\n","\n","    for i, (images, lebels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, lebels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        running_loss += loss.item()\n","\n","    print(f'[{epoch + 1}] loss: {running_loss / n_total_steps:.3f}')\n","\n","print('Finished Training')\n","PATH = './models'\n","torch.save(model.state_dict(), PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54mTVEnxu7N5","executionInfo":{"status":"ok","timestamp":1708327175301,"user_tz":480,"elapsed":371695,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"8ee42d1d-0564-45b8-f5d3-43f10080ed6b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] loss: 1.457\n","[2] loss: 1.080\n","[3] loss: 0.910\n","[4] loss: 0.806\n","[5] loss: 0.730\n","[6] loss: 0.666\n","[7] loss: 0.611\n","[8] loss: 0.565\n","[9] loss: 0.521\n","[10] loss: 0.480\n","Finished Training\n"]}]},{"cell_type":"code","source":["loaded_model = ConvNet()\n","loaded_model.load_state_dict(torch.load(PATH)) # it takes the loaded dictionary, not the path file itself\n","loaded_model.to(device)\n","loaded_model.eval()\n","\n","with torch.no_grad():\n","    n_correct = 0\n","    n_correct2 = 0\n","    n_samples = len(test_loader.dataset)\n","    # print(n_samples)\n","\n","    for images, lebels in test_loader:\n","        images = images.to(device)\n","        lebels = labels.to(device)\n","        outputs = model(images)\n","\n","        # max returns (value, index)\n","        _, predicted = torch.max(outputs, 1)\n","        n_correct += (predicted == labels).sum().item()\n","\n","        outputs2 = loaded_model(images)\n","        _, predicted2 = torch.max(outputs2, 1)\n","        n_correct2 += (predicted2 == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print(f'Accuracy of the model: {acc} %')\n","\n","    acc = 100.0 * n_corrects / n_samples\n","    print(f'Accuracy of the loaded model: {acc} %')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"zhxPun6CKWnY","executionInfo":{"status":"error","timestamp":1708327648724,"user_tz":480,"elapsed":6208,"user":{"displayName":"Tzechung Kao","userId":"14252687860553419910"}},"outputId":"c6d29802-fcd5-404b-b093-8e233cb56095"},"execution_count":8,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8a6fb40cc3ab>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# max returns (value, index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 0"]}]}]}